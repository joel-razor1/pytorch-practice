{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0038, -0.0315, -0.0292,  ..., -0.0355,  0.0035, -0.0034],\n",
      "        [-0.0290, -0.0045,  0.0034,  ..., -0.0250, -0.0054,  0.0289],\n",
      "        [-0.0028, -0.0108,  0.0018,  ..., -0.0289,  0.0205,  0.0190],\n",
      "        ...,\n",
      "        [-0.0336,  0.0292, -0.0334,  ..., -0.0113, -0.0203, -0.0048],\n",
      "        [-0.0176, -0.0247,  0.0236,  ...,  0.0029,  0.0254,  0.0159],\n",
      "        [-0.0006, -0.0262,  0.0302,  ...,  0.0112, -0.0103,  0.0349]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0024,  0.0292,  0.0163, -0.0317,  0.0230,  0.0159,  0.0185,  0.0112,\n",
      "         0.0142,  0.0163,  0.0048,  0.0292, -0.0221, -0.0188, -0.0220,  0.0340,\n",
      "        -0.0161,  0.0129, -0.0171, -0.0139,  0.0222,  0.0180,  0.0189, -0.0190,\n",
      "        -0.0162,  0.0151,  0.0104,  0.0022, -0.0356,  0.0179, -0.0281,  0.0184,\n",
      "        -0.0279, -0.0091,  0.0164, -0.0288,  0.0251, -0.0267,  0.0046, -0.0201,\n",
      "        -0.0061,  0.0215,  0.0293, -0.0026, -0.0017,  0.0331, -0.0040,  0.0189,\n",
      "         0.0296, -0.0062, -0.0016, -0.0122,  0.0331,  0.0141, -0.0284, -0.0256,\n",
      "         0.0118, -0.0152,  0.0331, -0.0148, -0.0245,  0.0178, -0.0351,  0.0301,\n",
      "        -0.0107,  0.0171,  0.0167,  0.0231,  0.0018, -0.0038,  0.0234,  0.0116,\n",
      "         0.0127, -0.0017,  0.0029,  0.0058,  0.0187, -0.0078,  0.0320,  0.0269,\n",
      "        -0.0243,  0.0056,  0.0299, -0.0139,  0.0278, -0.0284,  0.0275,  0.0035,\n",
      "        -0.0124,  0.0028, -0.0266,  0.0289, -0.0302, -0.0015, -0.0008, -0.0002,\n",
      "         0.0219,  0.0137,  0.0178,  0.0026, -0.0132, -0.0106,  0.0145,  0.0026,\n",
      "         0.0032, -0.0162, -0.0131, -0.0299, -0.0184, -0.0125, -0.0237, -0.0174,\n",
      "        -0.0016, -0.0191, -0.0260, -0.0039, -0.0316, -0.0333,  0.0148, -0.0249,\n",
      "        -0.0301, -0.0172,  0.0207,  0.0168, -0.0346, -0.0355, -0.0006, -0.0026],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.9698e-03, -8.5644e-03,  6.7509e-03,  ..., -5.3204e-03,\n",
       "          1.9077e-03,  1.4365e-02],\n",
       "        [-2.6273e-02, -2.2826e-05, -1.6398e-02,  ...,  4.5251e-03,\n",
       "          1.0175e-02,  3.3158e-03],\n",
       "        [-2.7419e-02,  2.2249e-02, -5.9541e-03,  ...,  3.3355e-03,\n",
       "          1.1873e-02, -6.3568e-03],\n",
       "        ...,\n",
       "        [ 6.6659e-03,  2.3171e-03,  7.3652e-03,  ...,  2.6081e-03,\n",
       "         -4.6001e-03, -5.1600e-03],\n",
       "        [ 8.1234e-04, -7.2715e-04,  3.7077e-03,  ...,  1.8657e-02,\n",
       "         -1.7465e-03,  8.1238e-03],\n",
       "        [-8.1834e-03,  1.2864e-02, -1.0752e-02,  ...,  1.4706e-03,\n",
       "         -1.4981e-02,  9.4008e-03]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "# Flatten the input images\n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "# Create parameters\n",
    "w1 = torch.randn(784, 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 10)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "h = activation(torch.mm(inputs, w1) + b1)\n",
    "\n",
    "out = torch.mm(h, w2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "## Solution\n",
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
    "\n",
    "probabilities = softmax(out)\n",
    "\n",
    "# Does it have the right shape? Should be (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Does it sum to 1?\n",
    "print(probabilities.sum(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'helper' has no attribute 'view_classify'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-07a71317f718>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimg_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'helper' has no attribute 'view_classify'"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to automatically get batch size\n",
    "\n",
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "\n",
    "img = images[img_idx]\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
